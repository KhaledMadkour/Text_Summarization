{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocessing.ipynb","provenance":[],"collapsed_sections":["bjY-wlRhhhEv"],"toc_visible":true,"authorship_tag":"ABX9TyMWcEVDXObLvJcMFm7Jd9kB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"MVyPAPuWa9ug","colab_type":"code","outputId":"44e51ffa-05f0-4c56-e712-dc33aaa546fd","executionInfo":{"status":"ok","timestamp":1586041206022,"user_tz":-120,"elapsed":19585,"user":{"displayName":"Khaled Madkour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPvs_OAOTDB4G7C7AzbTGAs1ucY_3qGHize-LvDA=s64","userId":"04599462734331178570"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Ieim7iTMv0d","colab_type":"text"},"source":["# **Preprocessing**  \n","I decided to make a customized preprocessing Function to be used in different problems.  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pc2JyNXqihBA","colab_type":"text"},"source":["By customized I mean you can choose the cleaning functions you want applied on your text or even choose between 2 different languages to clean (Arabic or English).  \n","\n","The function dafault Arguments:\n","\n","* lang = 'en' :**Works on English Language, 'ar' for Arabic** \n","* rm_extras = True :**Remove punctuations, numbers and special characters**\n","* lower = True :**Change English texts to lowercase**\n","* stopwords = True :**Remove Stopwords**\n","* stemming = True :**Stem texts**\n","* rm_contraction = True :**Maps contracted words to it's original**\n","* rm_tashkel = True :**Remove Tashkel From arabic texts**\n","* rm_short = False :**Remove Tashkel From arabic texts**\n","\n","you can change any of the True above arguments to false to prevent that particular cleaning.\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"EIYv1IIFfkcg","colab_type":"text"},"source":["## **Downloading libraries And importing stopwords**"]},{"cell_type":"markdown","metadata":{"id":"ULtA5Uh0ipe1","colab_type":"text"},"source":["The following code run once to download needed libraries and read the customized arabic stop words.\n","Make sure to change directory of text file of stop words to your downloaded path.\n","\n","The arabic stop words have been obtained from https://github.com/mohataher/arabic-stop-words"]},{"cell_type":"code","metadata":{"id":"WvmAxYzQdVQz","colab_type":"code","outputId":"e96882bf-1ca0-410e-d8a0-e57ca4f4ac96","executionInfo":{"status":"ok","timestamp":1586049499381,"user_tz":-120,"elapsed":8162,"user":{"displayName":"Khaled Madkour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPvs_OAOTDB4G7C7AzbTGAs1ucY_3qGHize-LvDA=s64","userId":"04599462734331178570"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["!pip install PyArabic\n","!pip install tashaphyne\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Read arabic stopwords text file.\n","filepath = \"/content/drive/My Drive/Colab Notebooks/Text_Summarization_Task/Word_Embedding/Arabic_Stopwords.txt\"\n","f = open(filepath, 'r')\n","stop_word_comp = [line.rstrip('\\n') for line in f.readlines()]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: PyArabic in /usr/local/lib/python3.6/dist-packages (0.6.7)\n","Requirement already satisfied: tashaphyne in /usr/local/lib/python3.6/dist-packages (0.3.4.1)\n","Requirement already satisfied: pyarabic in /usr/local/lib/python3.6/dist-packages (from tashaphyne) (0.6.7)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HwH0JlVjgRDl","colab_type":"text"},"source":["## **Import the Libraries**"]},{"cell_type":"code","metadata":{"id":"gS1LBm79Ubhj","colab_type":"code","colab":{}},"source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import pyarabic.araby as araby\n","from tashaphyne.stemming import ArabicLightStemmer\n","from textblob import TextBlob\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEHwAqhzgqfO","colab_type":"text"},"source":["## Functions Used in the main preprocess function separated to reduce the complexity of the function\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"LfhW7Kgvcydf","colab_type":"code","colab":{}},"source":["contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n","\n","                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n","\n","                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n","\n","                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n","\n","                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n","\n","                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n","\n","                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n","\n","                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n","\n","                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n","\n","                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n","\n","                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n","\n","                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n","\n","                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n","\n","                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n","\n","                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n","\n","                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n","\n","                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n","\n","                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n","\n","                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n","\n","                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n","\n","                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n","\n","                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n","\n","                           \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","    \n","def arabic_stem(text):\n","    ArListem = ArabicLightStemmer()\n","    zen = TextBlob(text)\n","    words = zen.words\n","    cleaned = list()\n","    for w in words:\n","        ArListem.light_stem(w)\n","        cleaned.append(ArListem.get_root())\n","    return \" \".join(cleaned)\n","\n","def normalizeArabic(text):\n","    text = text.strip()\n","    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n","    text = re.sub(\"ى\", \"ي\", text)\n","    text = re.sub(\"ؤ\", \"ء\", text)\n","    text = re.sub(\"ئ\", \"ء\", text)\n","    text = re.sub(\"ة\", \"ه\", text)\n","    noise = re.compile(\"\"\" ّ    | # Tashdid\n","                             َ    | # Fatha\n","                             ً    | # Tanwin Fath\n","                             ُ    | # Damma\n","                             ٌ    | # Tanwin Damm\n","                             ِ    | # Kasra\n","                             ٍ    | # Tanwin Kasr\n","                             ْ    | # Sukun\n","                             ـ     # Tatwil/Kashida\n","                         \"\"\", re.VERBOSE)\n","    text = re.sub(noise, '', text)\n","    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n","    return araby.strip_tashkeel(text)\n","\n","\n","def remove_stop_words(text , lang ):\n","    if (lang == 'en'):\n","       stop_words = stopwords.words('english')\n","    else:\n","       stop_words = stopwords.words('arabic')\n","    \n","    zen = TextBlob(text)\n","    words = zen.words\n","    return \" \".join([w for w in words if not w in stop_words and not w in stop_word_comp and len(w) >= 2])\n","\n","def remove_short_words(text):\n","  not_so_short_text = ''\n","  for t in text.split(' '):\n","    if len(t) >= 3:\n","      not_so_short_text = not_so_short_text + ' ' + t\n","  return not_so_short_text\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVrRNFmYgZV0","colab_type":"text"},"source":["## Main Preprocessing Function"]},{"cell_type":"code","metadata":{"id":"d49F3kppgaME","colab_type":"code","colab":{}},"source":["def preprocess(text, lang=\"en\" , rm_extras = True , lower = True , stopwords = True , stemming = True , rm_tashkel = True  , rm_contraction = True , rm_short = False):\n","    if (lang == \"en\"):\n","      \n","      # make alphabets lowercase\n","      if(lower):\n","        text  = text.lower()\n","\n","      #remove stopwords\n","      if(stopwords):\n","        text = remove_stop_words(text , 'en')\n","\n","      # Expand contrated words\n","      if (rm_contraction):\n","        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n","\n","      # remove punctuations, numbers and special characters\n","      if(rm_extras):\n","        text  = re.sub(\"[^a-zA-Z]\", \" \" , text)\n","\n","      # Stemming texts\n","      if(stemming):\n","        porter = PorterStemmer()\n","        text = porter.stem(text)\n","      \n","      # Removing short texts\n","      if(rm_short):\n","        text = remove_short_words(text)\n","\n","      #remove spaces from endlines\n","      text = text.strip()\n","\n","    if (lang==\"ar\"):\n","      # remove punctuations, numbers and special characters\n","      if(rm_extras):\n","        text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n","        ## remove extra whitespace\n","        text = re.sub('\\s+', ' ', text)\n","        ## Remove numbers\n","        text = re.sub(\"\\d+\", \" \", text)\n","      \n","      ## Remove stop words\n","      if(stopwords):\n","        text = remove_stop_words(text , 'ar')\n","      \n","      if(rm_tashkel):\n","        ## Remove Tashkeel\n","        text = normalizeArabic(text)\n","        text = re.sub('[A-Za-z]+',' ',text)\n","        text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n","\n","      ## Remove extra whitespace\n","      text = re.sub('\\s+', ' ', text)  \n","\n","      ## Stemming\n","      if(stemming):\n","        text = arabic_stem(text)\n","\n","      #remove spaces from endlines\n","      text = text.strip()\n","      \n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjY-wlRhhhEv","colab_type":"text"},"source":["# **Reading From wikipedia**\n","This function is simply used to read a wikipedia url, Clean it and return list of sentences.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cj2i2xDk13Eo","colab_type":"text"},"source":["we begin by importing the essential libraries for fetching data from the web page. The BeautifulSoup library is used for parsing the page while the urllib library is used for connecting to the page and retrieving the HTML.\n","\n","BeautifulSoup converts the incoming text to Unicode characters and the outgoing text to UTF-8 characters, saving you the hassle of managing different charset encodings while scraping text from the web.\n","\n","We’ll use the urlopen function from the urllib.request utility to open the web page. Then, we’ll use the read function to read the scraped data object. For parsing the data, we’ll call the BeautifulSoup object and pass two parameters to it; that is, the article_read and the html.parser.\n","\n","The find_all function is used to return all the ``` <p> ``` elements present in the HTML. Furthermore, using .text enables us to select only the texts found within the ``` <p> ``` elements."]},{"cell_type":"code","metadata":{"id":"RD9lSprahnUn","colab_type":"code","colab":{}},"source":["import bs4 as BeautifulSoup\n","import urllib.request\n","\n","def read_wiki(url):\n","  #fetching the content from the URL\n","  fetched_data = urllib.request.urlopen(url)\n","  article_read = fetched_data.read()\n","\n","  #parsing the URL content and storing in a variable\n","  article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n","\n","  #returning <p> tags\n","  paragraphs = article_parsed.find_all('p')\n","\n","  article_content = ''\n","  \n","  #looping through the paragraphs and adding them to the variable\n","  for p in paragraphs:  \n","      article_content += p.text\n","  return article_content"],"execution_count":0,"outputs":[]}]}